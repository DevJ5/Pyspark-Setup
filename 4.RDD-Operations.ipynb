{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"C:/Program Files/Spark\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"lab\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = (\n",
    "    \"C:/Users//AppData/Local/Programs/Python/Python310/python.exe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"RDD-Demo\").getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Everything is actions or transformations\n",
    "\n",
    "# Collect action\n",
    "rdd.collect()\n",
    "# Transformation 1: Double each number\n",
    "doubled_rdd = rdd.map(lambda x: x * 2)\n",
    "print(doubled_rdd.collect())  # Output: [2, 4, 6, 8, 10]\n",
    "\n",
    "# Transformation 2: Sum the numbers\n",
    "sum_result = rdd.reduce(lambda x, y: x + y)\n",
    "print(sum_result)  # Output: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list of tuples\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35), (\"Alice\", 40)]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements of the rdd:  [('Alice', 25), ('Bob', 30), ('Charlie', 35), ('Alice', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Collect action: Retrieve all elements of the RDD\n",
    "print(\"All elements of the rdd: \", rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs Operation: Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of elements in rdd:  4\n"
     ]
    }
   ],
   "source": [
    "# Count action: Count the number of elements in the RDD\n",
    "count = rdd.count()\n",
    "print(\"The total number of elements in rdd: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element of the rdd:  ('Alice', 25)\n"
     ]
    }
   ],
   "source": [
    "# First action: Retrieve the first element of the RDD\n",
    "first_element = rdd.first()\n",
    "print(\"The first element of the rdd: \", first_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreach action: Print each element of the RDD\n",
    "# The print statements are executed on the worker nodes in the cluster, \n",
    "# but their outputs are typically not captured by the driver, and you won’t see them in the driver’s console.\n",
    "rdd.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs Operation: Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd with uppercase name:  [('ALICE', 25), ('BOB', 30), ('CHARLIE', 35), ('ALICE', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Map transformation: Convert name to uppercase\n",
    "mapped_rdd = rdd.map(lambda x: (x[0].upper(), x[1]))\n",
    "result = mapped_rdd.collect()\n",
    "print(\"rdd with uppercase name: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charlie', 35), ('Alice', 40)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter transformation: Filter records where age is greater than 30\n",
    "filtered_rdd = rdd.filter(lambda x: x[1] > 30)\n",
    "filtered_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charlie', 35), ('Bob', 30), ('Alice', 65)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReduceByKey transformation: Calculate the total age for each name\n",
    "reduced_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 40), ('Charlie', 35), ('Bob', 30), ('Alice', 25)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SortBy transformation: Sort the RDD by age in descending order\n",
    "sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "sorted_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save RDDs to text file and read RDDs from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save action: Save the RDD to a text file\n",
    "rdd.saveAsTextFile(\"output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rdd from text file\n",
    "rdd_text = spark.sparkContext.textFile(\"output.txt\")\n",
    "rdd_text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
